ğŸ©º RAG-based Medical FAQ Chatbot
ğŸ“– Overview

This project implements a Retrieval-Augmented Generation (RAG) chatbot for answering medical FAQs.
It uses:

SentenceTransformers (all-MiniLM-L6-v2) for embeddings

ChromaDB for vector storage & retrieval

Groq API for fast LLM-powered response generation

FastAPI to serve the chatbot backend

Streamlit to provide a simple chat-based UI

âš™ï¸ Tech Stack

Python 3.9+

FastAPI
 â€“ API framework

ChromaDB
 â€“ vector database

SentenceTransformers
 â€“ embeddings

Groq API
 â€“ LLM inference

Streamlit
 â€“ frontend UI

Uvicorn
 â€“ ASGI server

ğŸ“‚ Project Structure
.
â”œâ”€â”€ app.py               # FastAPI backend with RAG pipeline
â”œâ”€â”€ preprocess.py        # Preprocessing script: builds ChromaDB index
â”œâ”€â”€ ui.py                # Streamlit chat frontend
â”œâ”€â”€ Medical_FAQ.csv      # Dataset (Question, Answer format)
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ chroma_persist/      # ChromaDB persistent index (auto-created)

ğŸ“¦ Installation

Clone repo & create virtual environment

git clone <your-repo-url>
cd rag-medical-faq
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate


Install dependencies

pip install -r requirements.txt


Set environment variables
Create a .env file in the project root:

GROQ_API_KEY=your_groq_api_key

ğŸ› ï¸ Usage
1. Preprocess dataset into ChromaDB

Ensure your dataset Medical_FAQ.csv has columns:

Question

Answer

Then run:

python preprocess.py


This will:

Read the CSV

Encode Q&A pairs into embeddings

Store them in a persistent ChromaDB collection (./chroma_persist/)

2. Start FastAPI backend
uvicorn app:app --reload


Backend runs at:
ğŸ‘‰ http://localhost:8000

Example request:

curl -X POST "http://127.0.0.1:8000/query" \
     -H "Content-Type: application/json" \
     -d '{"question": "Who is at risk for Lymphocytic Choriomeningitis (LCM)?"}'


Response:

{
  "question": "Who is at risk for Lymphocytic Choriomeningitis (LCM)?",
  "answer": "Individuals of all ages who come into contact with urine, feces, saliva, or blood of wild mice..."
}

3. Run Streamlit frontend
streamlit run ui.py


Open browser: ğŸ‘‰ http://localhost:8501

You can now chat with the bot interactively.

ğŸ§© Design Choices

Embeddings: Local SentenceTransformer (all-MiniLM-L6-v2) â†’ avoids API cost & latency.

Vector DB: ChromaDB PersistentClient â†’ ensures data survives restarts.

LLM: Groq (openai/gpt-oss-20b or llama-3.1-8b/70b) â†’ fast inference.

Backend: FastAPI â†’ clean, modular, easy to extend.

Frontend: Streamlit â†’ quick, no-JS chat UI.